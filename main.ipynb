{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall results\n",
    "\n",
    "## 1. F1-score results in the NL setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def check_str(res_text):\n",
    "    left_count = 0\n",
    "    right_count = 0\n",
    "    for i in res_text:\n",
    "        if i == '[':\n",
    "            left_count += 1\n",
    "        if i == ']':\n",
    "            right_count += 1\n",
    "        return res_text[res_text.index('['): res_text.index(']')+1]\n",
    "                \n",
    "def eval_f1_score(dataset):\n",
    "    with open(rf\"./data/dataset/test/{dataset}_test_data.json\", 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    for model in ['GPT 3.5', 'GPT 4', 'GPT 3.5 (all knobs)', 'GPT 4 (all knobs)', 'Andromeda GPT 3.5', 'Andromeda GPT 4']:\n",
    "        path = rf'./data/reasoning_results/{dataset}/{model}'\n",
    "        filelist = os.listdir(path)\n",
    "\n",
    "        all_f1 = 0\n",
    "        for qid_json in filelist:\n",
    "            qid = qid_json.replace(\".json\", \"\")\n",
    "            try:\n",
    "                with open(f'{path}/{qid_json}', 'r') as f:\n",
    "                    res = json.load(f)\n",
    "                            \n",
    "                res_text = res['res']\n",
    "                if '[' in res_text:\n",
    "                    res_text = check_str(res_text)\n",
    "                res = eval(res_text)\n",
    "                gt = test_data[qid_json.split('.')[0]]['parameter']\n",
    "                recall = len(set(res) & set(gt)) / len(gt)\n",
    "                if len(res) != 0:\n",
    "                    precision = len(set(res) & set(gt)) / len(res)\n",
    "                else:\n",
    "                    precision = 0\n",
    "\n",
    "                if recall + precision == 0:\n",
    "                    f1 = 0\n",
    "                else:\n",
    "                    f1 = 2 * recall * precision / (recall + precision)\n",
    "\n",
    "                all_f1 += f1\n",
    "            except:\n",
    "                continue\n",
    "        print(model, all_f1 / len(filelist))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT 3.5 0.22146041860099785\n",
      "GPT 4 0.2221834933903899\n",
      "GPT 3.5 (all knobs) 0.3075098218905717\n",
      "GPT 4 (all knobs) 0.33162667182893907\n",
      "Andromeda GPT 3.5 0.34833463444002094\n",
      "Andromeda GPT 4 0.44095292863372215\n"
     ]
    }
   ],
   "source": [
    "eval_f1_score('mysql_so')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT 3.5 0.07296669931401181\n",
      "GPT 4 0.04821577511778364\n",
      "GPT 3.5 (all knobs) 0.2691097268633844\n",
      "GPT 4 (all knobs) 0.25290407185858077\n",
      "Andromeda GPT 3.5 0.3823391780176497\n",
      "Andromeda GPT 4 0.44868784844040044\n"
     ]
    }
   ],
   "source": [
    "eval_f1_score('mysql_forum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT 3.5 0.2005854410286543\n",
      "GPT 4 0.16086744639376221\n",
      "GPT 3.5 (all knobs) 0.29897868452808435\n",
      "GPT 4 (all knobs) 0.38954330270119736\n",
      "Andromeda GPT 3.5 0.32105263157894737\n",
      "Andromeda GPT 4 0.39788359788359795\n"
     ]
    }
   ],
   "source": [
    "eval_f1_score('pg_so')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) SuccessRate results in the Runnable setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_SuccessRate():\n",
    "    with open(rf\"./data/dataset/test/mysql_run_test_data.json\", 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    with open('./data/manual_evaluation_on_runnable_setting.json', 'r') as f:\n",
    "        manual_evaluation = json.load(f)\n",
    "\n",
    "    for model in ['GPT 3.5', 'GPT 4', 'GPT 3.5 (all knobs)', 'GPT 4 (all knobs)', 'Andromeda GPT 3.5', 'Andromeda GPT 4']:\n",
    "        path = rf'./data/reasoning_results/mysql_run/{model}'\n",
    "        filelist = os.listdir(path)\n",
    "        success_rate = 0\n",
    "        for qid_json in filelist:\n",
    "            qid = qid_json.replace(\".json\", \"\")\n",
    "            if manual_evaluation[model][qid] == 1:\n",
    "                success_rate += 1\n",
    "        print(model, success_rate / len(filelist))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT 3.5 0.49019607843137253\n",
      "GPT 4 0.47058823529411764\n",
      "GPT 3.5 (all knobs) 0.5098039215686274\n",
      "GPT 4 (all knobs) 0.5686274509803921\n",
      "Andromeda GPT 3.5 0.7647058823529411\n",
      "Andromeda GPT 4 0.7058823529411765\n"
     ]
    }
   ],
   "source": [
    "eval_SuccessRate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
